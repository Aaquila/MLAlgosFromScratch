{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "Mo2xEMjBsPw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    The dataset used is a subset of MNIST digit classification dataset.\n",
        "\n",
        "    The model is implemented from scratch with 4 different layers of variable number of nodes."
      ],
      "metadata": {
        "id": "JxaymTK6sSFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data & library import"
      ],
      "metadata": {
        "id": "j1-esqBPsizz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SIgOxQSPImou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c506d83c-076b-47a0-dcee-ae1e44962f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 19665775.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 623693.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5631704.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4129748.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True,)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sql2ovjHc2nE",
        "outputId": "9d11a9e6-1d9e-41fa-caed-86e9a46abfec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 1000, 1: 1000, 2: 1000}\n",
            "{0: 333, 1: 333, 2: 333}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3000, 784), (3000, 1), (999, 784), (999, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "def obtain_subset(dataset,num_sample, num_classes=2):\n",
        "    ###num_sample should be a multiple of num_classes\n",
        "    new_x = np.zeros((num_sample, 784))\n",
        "    new_y = np.zeros((num_sample,1),dtype=np.int8)\n",
        "\n",
        "    lab = {}\n",
        "    for i in range(num_classes):\n",
        "        lab[i] = 0\n",
        "\n",
        "    j = 0\n",
        "    for i in range(len(train_dataset)):\n",
        "        if train_dataset[i][1] in lab and lab[train_dataset[i][1]] < num_sample/num_classes:\n",
        "            lab[train_dataset[i][1]] += 1\n",
        "            new_x[j,:] = train_dataset[i][0].numpy().reshape(-1)\n",
        "            new_y[j] = train_dataset[i][1]\n",
        "            j+=1\n",
        "        if j > num_sample:\n",
        "            break\n",
        "    print(lab)\n",
        "    return new_x, new_y\n",
        "\n",
        "tr_x, tr_y = obtain_subset(train_dataset,3000,3)\n",
        "tst_x, tst_y = obtain_subset(test_dataset,999,3)\n",
        "\n",
        "tr_x.shape, tr_y.shape, tst_x.shape, tst_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation"
      ],
      "metadata": {
        "id": "A7qfoHexsm-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNNs:\n",
        "    def __init__(self, num_classes=2, num_layers = 4, num_epochs=10, batch_size=64,\n",
        "                 lr=0.001, step=10, step_rate=0.1):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layer= num_layers\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.step = step\n",
        "        self.step_rate = step_rate\n",
        "\n",
        "    def fit(self,x,y):\n",
        "        #initialization\n",
        "        self.initialize_layers(x.shape[1])\n",
        "\n",
        "\n",
        "        loss = np.inf\n",
        "        vt = [np.zeros((self.w[i].shape)) for i in range(self.num_layer)]\n",
        "        mt = [np.zeros((self.w[i].shape)) for i in range(self.num_layer)]\n",
        "        beta1, beta2 =  0.9, 0.9\n",
        "        epsilon = 1e-8\n",
        "        batches = x.shape[0]//self.batch_size\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "\n",
        "            rand_ind = np.random.permutation(x.shape[0])\n",
        "            x_rand = x[rand_ind,:]\n",
        "            y_rand = y[rand_ind,:]\n",
        "\n",
        "            for j in range(batches):\n",
        "                begin = j*self.batch_size\n",
        "                end = (j+1)*self.batch_size\n",
        "                if end > x.shape[0]:\n",
        "                    continue\n",
        "\n",
        "                x_new = x_rand[begin:end,:]\n",
        "                y_new = y_rand[begin:end,:]\n",
        "\n",
        "                #forward compute\n",
        "                self.compute_forward(x_new)\n",
        "\n",
        "\n",
        "                #compute gradients via backpropogation\n",
        "                self.compute_backward(x_new,y_new)\n",
        "\n",
        "                #update weights\n",
        "                for j in range(self.num_layer):\n",
        "                    self.w[j] -= self.lr * self.dw[j]\n",
        "                    self.b[j] -= self.lr * self.db[j]\n",
        "\n",
        "                    #Adam optimizer\n",
        "                    #mt[j] = beta1*mt[j] + (1-beta1)*self.dw[j]\n",
        "                    #vt[j] = beta2*vt[j] + (1-beta2)*self.dw[j]**2\n",
        "                    #self.w[j] -= (self.lr/np.sqrt(vt[j] + epsilon) )*mt[j]\n",
        "                    #self.b[j] -= self.lr * self.db[j]\n",
        "\n",
        "            #compute loss\n",
        "            prev_loss = loss\n",
        "            self.compute_forward(x_rand)\n",
        "            loss = np.mean(self.loss_cce(y_rand, self.a[-1]))\n",
        "            print(f'Epoch: {i} LR {self.lr} Loss: {loss}')\n",
        "\n",
        "            if loss > 0.3 + prev_loss:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "            if (i+1)%self.step == 0:\n",
        "                self.lr *= self.step_rate\n",
        "\n",
        "\n",
        "    def predict(self,x):\n",
        "        self.compute_forward(x)\n",
        "        return np.argmax(self.a[-1],axis=1,keepdims=True)\n",
        "\n",
        "    def initialize_one_layer(self,a,b):\n",
        "        w = np.random.randn(a,b) #i/p - m x 728, w - 728 x num_nodes, o/p - m x 10, b - 1 x num_nodes\n",
        "        #*np.sqrt(1./a)\n",
        "        b = np.zeros((1,b))\n",
        "        return w, b\n",
        "\n",
        "    def initialize_layers(self,num_feats):\n",
        "        num_nodes = [500,100,64,self.num_classes]\n",
        "        self.w, self.b = [], []\n",
        "\n",
        "        #initialization\n",
        "        print('Initialiazing weights')\n",
        "        for i in range(self.num_layer):\n",
        "            if i==0:\n",
        "                nodes = num_feats\n",
        "            else:\n",
        "                nodes = num_nodes[i-1]\n",
        "            w,b = self.initialize_one_layer(nodes,num_nodes[i])\n",
        "            print(f'Layer {i} :, w - {w.shape}, b - {b.shape}')\n",
        "            self.w.append(w)\n",
        "            self.b.append(b)\n",
        "        print()\n",
        "\n",
        "    def compute_forward(self,x):\n",
        "        self.a = [[] for _ in range(self.num_layer)]\n",
        "        self.z = [[] for _ in range(self.num_layer)]\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "            if i==0:\n",
        "                aa = x\n",
        "            else:\n",
        "                aa = self.a[i-1]\n",
        "            self.z[i] = np.dot(aa,self.w[i]) + self.b[i]\n",
        "            self.a[i] = self.softmax(self.z[i])\n",
        "\n",
        "\n",
        "    def compute_backward(self,x,y):\n",
        "        self.dw = [[] for _ in range(self.num_layer)]\n",
        "        self.db = [[] for _ in range(self.num_layer)]\n",
        "        one_hot = self.get_one_hot(y)\n",
        "\n",
        "        for i in range(self.num_layer-1,-1,-1):\n",
        "            if i==self.num_layer-1:\n",
        "                dz = self.a[-1] - one_hot\n",
        "                assert dz.shape == self.z[i].shape\n",
        "            else:\n",
        "                dz = np.dot(dz, self.w[i+1].T) * self.grad_softmax(self.z[i]) #gz - 1000 x 100, w - 100 x 10, dz - 1000 x 10, new dz - 1000 x 100\n",
        "                assert dz.shape == self.z[i].shape\n",
        "            if i==0:\n",
        "                self.dw[i] = (1/x.shape[0]) * np.dot(x.T,dz)  #x - 1000 x 784, dz - 1000 x 500\n",
        "            else:\n",
        "                self.dw[i] = (1/x.shape[0]) * np.dot(self.a[i-1].T,dz)\n",
        "\n",
        "            self.db[i] = (1/x.shape[0]) * np.sum(dz,axis=0)\n",
        "\n",
        "    def loss_cce(self,y,pred):\n",
        "        one_hot = self.get_one_hot(y)\n",
        "        return -np.mean(one_hot*np.log(pred),axis=0)\n",
        "\n",
        "    def softmax(self,x,temp=1.05):\n",
        "        #To avoid loss becoming nan when predictions are too high or too low, resulting in log(0) or log(1)\n",
        "        t = np.exp(x/temp)\n",
        "        return t/(np.sum(t,axis=1, keepdims=True) )\n",
        "\n",
        "    def grad_softmax(self,x):\n",
        "        t = self.softmax(x)\n",
        "        return t*(1-t)\n",
        "\n",
        "    def get_one_hot(self,y):\n",
        "        one_hot = np.zeros((y.shape[0],self.num_classes))\n",
        "        for i in range(y.shape[0]):\n",
        "            one_hot[i, int(y[i].item())] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def acc(self,y,pred):\n",
        "        return np.mean(y==pred)*100"
      ],
      "metadata": {
        "id": "841gtdjSABpJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5m0Eb8NmzoD",
        "outputId": "7dc65598-74c7-45b3-821e-04cf6d6bebad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Initialiazing weights\n",
            "Layer 0 :, w - (784, 500), b - (1, 500)\n",
            "Layer 1 :, w - (500, 100), b - (1, 100)\n",
            "Layer 2 :, w - (100, 64), b - (1, 64)\n",
            "Layer 3 :, w - (64, 3), b - (1, 3)\n",
            "\n",
            "Epoch: 0 LR 3 Loss: 0.36825319453061\n",
            "Epoch: 1 LR 3 Loss: 0.3658933982594727\n",
            "Epoch: 2 LR 3 Loss: 0.37053120036013837\n",
            "Epoch: 3 LR 3 Loss: 0.3635354024251159\n",
            "Epoch: 4 LR 3 Loss: 0.36264447121823834\n",
            "Epoch: 5 LR 3 Loss: 0.3500388907582326\n",
            "Epoch: 6 LR 3 Loss: 0.23106141259356805\n",
            "Epoch: 7 LR 3 Loss: 0.19116149674330432\n",
            "Epoch: 8 LR 3 Loss: 0.18234005386011287\n",
            "Epoch: 9 LR 3 Loss: 0.1277214491340815\n",
            "Epoch: 10 LR 3 Loss: 0.09964955747997391\n",
            "Epoch: 11 LR 3 Loss: 0.10219736001386186\n",
            "Epoch: 12 LR 3 Loss: 0.06855875071232308\n",
            "Epoch: 13 LR 3 Loss: 0.09650873482197075\n",
            "Epoch: 14 LR 3 Loss: 0.06628796822870356\n",
            "Epoch: 15 LR 3 Loss: 0.0731376038800537\n",
            "Epoch: 16 LR 3 Loss: 0.054205839863626544\n",
            "Epoch: 17 LR 3 Loss: 0.054840671019798615\n",
            "Epoch: 18 LR 3 Loss: 0.06580979616207762\n",
            "Epoch: 19 LR 3 Loss: 0.050778930516030406\n",
            "Epoch: 20 LR 1.5 Loss: 0.04850689423836929\n",
            "Epoch: 21 LR 1.5 Loss: 0.04526686639013581\n",
            "Epoch: 22 LR 1.5 Loss: 0.0407092888097122\n",
            "Epoch: 23 LR 1.5 Loss: 0.050287925362855916\n",
            "Epoch: 24 LR 1.5 Loss: 0.05125559933200804\n",
            "Epoch: 25 LR 1.5 Loss: 0.06454443815511193\n",
            "Epoch: 26 LR 1.5 Loss: 0.04279526513627598\n",
            "Epoch: 27 LR 1.5 Loss: 0.04147163153549695\n",
            "Epoch: 28 LR 1.5 Loss: 0.04102579363321071\n",
            "Epoch: 29 LR 1.5 Loss: 0.049477149940878894\n",
            "Epoch: 30 LR 1.5 Loss: 0.038338506074680305\n",
            "Epoch: 31 LR 1.5 Loss: 0.03349702944587671\n",
            "Epoch: 32 LR 1.5 Loss: 0.032998765313596225\n",
            "Epoch: 33 LR 1.5 Loss: 0.03290833270076383\n",
            "Epoch: 34 LR 1.5 Loss: 0.03347417582332322\n",
            "Epoch: 35 LR 1.5 Loss: 0.031247261078127846\n",
            "Epoch: 36 LR 1.5 Loss: 0.031628543226956636\n",
            "Epoch: 37 LR 1.5 Loss: 0.041248491639611654\n",
            "Epoch: 38 LR 1.5 Loss: 0.03504294261962413\n",
            "Epoch: 39 LR 1.5 Loss: 0.032820335468111736\n",
            "Epoch: 40 LR 0.75 Loss: 0.03076942517180802\n",
            "Epoch: 41 LR 0.75 Loss: 0.028060202025467995\n",
            "Epoch: 42 LR 0.75 Loss: 0.027440642639860163\n",
            "Epoch: 43 LR 0.75 Loss: 0.02719437806655813\n",
            "Epoch: 44 LR 0.75 Loss: 0.027151032968445534\n",
            "Epoch: 45 LR 0.75 Loss: 0.02647856693583046\n",
            "Epoch: 46 LR 0.75 Loss: 0.02745097637673727\n",
            "Epoch: 47 LR 0.75 Loss: 0.026243575643908796\n",
            "Epoch: 48 LR 0.75 Loss: 0.02612849828905975\n",
            "Epoch: 49 LR 0.75 Loss: 0.026098527951846934\n",
            "Training model accuracy: 98.47 %\n",
            "Testing model accuracy: 98.7 %\n"
          ]
        }
      ],
      "source": [
        "model = DNNs(num_classes=3, num_layers = 4, num_epochs=50, batch_size=128, lr=3,step=20, step_rate=0.5)\n",
        "print('Training')\n",
        "model.fit(tr_x,tr_y)\n",
        "tr_pred =  model.predict(tr_x)\n",
        "print(f'Training model accuracy: {model.acc(tr_y,tr_pred):.4} %')\n",
        "tst_pred =  model.predict(tst_x)\n",
        "print(f'Testing model accuracy: {model.acc(tst_y,tst_pred):.4} %')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXsywTDt1pnkq6b/Nw04mi"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}